{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7DpqVRp0Wr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers PyPDF2"
      ],
      "metadata": {
        "id": "fkPJQua70Wom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "\n"
      ],
      "metadata": {
        "id": "2qSMKV-G0WlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L9D_SoYz90s"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"hkunlp/instructor-large\")\n",
        "\n",
        "sentences = [\n",
        "    \"That is a happy person\",\n",
        "    \"That is a happy dog\",\n",
        "    \"That is a very happy person\",\n",
        "    \"Today is a sunny day\"\n",
        "]\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities.shape)\n",
        "# [4, 4]"
      ],
      "metadata": {
        "id": "WFUA6Th70Miw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Setup device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")\n",
        "\n",
        "# Load embedding and language models\n",
        "embedding_model = SentenceTransformer('hkunlp/instructor-large', device=device)\n",
        "language_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
        "language_model = AutoModelForCausalLM.from_pretrained(\n",
        "   language_model_name,\n",
        "   device_map=\"auto\",\n",
        "   torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "# Extract text from PDF with improved cleaning\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "   reader = PdfReader(pdf_path)\n",
        "   full_text = \"\"\n",
        "   for page in reader.pages:\n",
        "       text = page.extract_text()\n",
        "       # Basic cleaning of extracted text\n",
        "       text = text.replace('\\n', ' ')\n",
        "       text = text.replace('  ', ' ')  # Remove double spaces\n",
        "       text = ' '.join(text.split())\n",
        "       full_text += text + \" \"\n",
        "   return full_text\n",
        "\n",
        "# Improved text splitting function\n",
        "def split_text(text, chunk_size=1000):\n",
        "   words = text.split()\n",
        "   chunks = []\n",
        "   current_chunk = []\n",
        "   current_length = 0\n",
        "\n",
        "   for word in words:\n",
        "       if current_length + len(word) > chunk_size:\n",
        "           chunks.append(' '.join(current_chunk))\n",
        "           current_chunk = [word]\n",
        "           current_length = len(word)\n",
        "       else:\n",
        "           current_chunk.append(word)\n",
        "           current_length += len(word) + 1  # +1 for space\n",
        "\n",
        "   if current_chunk:\n",
        "       chunks.append(' '.join(current_chunk))\n",
        "\n",
        "   return chunks\n",
        "\n",
        "# Create embeddings\n",
        "def create_embeddings(chunks):\n",
        "   return embedding_model.encode(chunks, convert_to_tensor=True)\n",
        "\n",
        "# Improved search function with debugging\n",
        "def search_relevant_chunks(question, chunks, embeddings, top_k=3):\n",
        "   question_embedding = embedding_model.encode(question, convert_to_tensor=True)\n",
        "   similarities = torch.matmul(embeddings, question_embedding.T).cpu().numpy()\n",
        "   top_indices = np.argsort(similarities, axis=0)[-top_k:][::-1]\n",
        "   selected_chunks = [chunks[idx] for idx in top_indices.flatten()]\n",
        "\n",
        "   # Debug print\n",
        "   print(\"\\nRelevant excerpts found:\")\n",
        "   for i, chunk in enumerate(selected_chunks, 1):\n",
        "       print(f\"\\nExcerpt {i}:\\n{chunk[:200]}...\")\n",
        "\n",
        "   return selected_chunks\n",
        "\n",
        "# Improved prompt creation\n",
        "def create_prompt(relevant_chunks, question):\n",
        "   context = \"\\n\\n\".join(relevant_chunks)\n",
        "   return f\"\"\"Based on the following excerpt from 'The Lightning Thief', please answer the question. If the information is not directly stated in the excerpt, please say so.\n",
        "\n",
        "Excerpt:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer (based only on the excerpt above): \"\"\"\n",
        "\n",
        "# Improved chat function\n",
        "def chat_with_books(pdf_path):\n",
        "   print(\"Loading and analyzing book...\")\n",
        "   try:\n",
        "       book_text = extract_text_from_pdf(pdf_path)\n",
        "       print(f\"Successfully extracted {len(book_text)} characters of text\")\n",
        "\n",
        "       chunks = split_text(book_text)\n",
        "       print(f\"Split into {len(chunks)} chunks\")\n",
        "\n",
        "       print(\"Creating embeddings...\")\n",
        "       embeddings = create_embeddings(chunks)\n",
        "       print(\"Embeddings created successfully!\")\n",
        "\n",
        "       print(\"\\nWelcome! Type 'exit' to end chat.\")\n",
        "       print(\"Type 'debug' to see the first chunk of text.\")\n",
        "\n",
        "       while True:\n",
        "           user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "           if user_input.lower() == \"exit\":\n",
        "               print(\"Goodbye!\")\n",
        "               break\n",
        "\n",
        "           if user_input.lower() == \"debug\":\n",
        "               print(\"\\nFirst chunk of text:\")\n",
        "               print(chunks[0][:500])\n",
        "               continue\n",
        "\n",
        "           relevant_chunks = search_relevant_chunks(user_input, chunks, embeddings)\n",
        "           prompt = create_prompt(relevant_chunks, user_input)\n",
        "\n",
        "           inputs = tokenizer(\n",
        "               prompt,\n",
        "               return_tensors=\"pt\",\n",
        "               truncation=True,\n",
        "               max_length=1024\n",
        "           ).to(device)\n",
        "\n",
        "           outputs = language_model.generate(\n",
        "               **inputs,\n",
        "               max_length=1024,\n",
        "               temperature=0.1,\n",
        "               top_p=0.9,\n",
        "               do_sample=True,\n",
        "               pad_token_id=tokenizer.eos_token_id,\n",
        "               num_return_sequences=1\n",
        "           )\n",
        "\n",
        "           response = tokenizer.decode(\n",
        "               outputs[:, inputs[\"input_ids\"].shape[-1]:][0],\n",
        "               skip_special_tokens=True\n",
        "           )\n",
        "           print(f\"\\nModel: {response}\")\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"An error occurred: {str(e)}\")\n",
        "       import traceback\n",
        "       print(traceback.format_exc())\n",
        "\n",
        "# Run program\n",
        "if __name__ == \"__main__\":\n",
        "   pdf_path = \"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\"  # Replace with actual book path\n",
        "   chat_with_books(pdf_path)"
      ],
      "metadata": {
        "id": "tCuGuUkbz-Xf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}